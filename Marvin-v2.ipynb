{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c63a7622-fa73-4cb8-ab6b-6ce61465280d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#!pip install paho-mqtt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "85cd4584-f4d8-4f2a-8672-802afa05fd54",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To talk with the MQTT server from the Discord Server\n",
    "import paho.mqtt.client as mqtt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b3beeb72-574a-4fbc-940f-4c83f1b206b7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Import to query wikipedia\n",
    "#from langchain.utilities import WikipediaAPIWrapper\n",
    "import wikipedia\n",
    "from langchain.utilities import WikipediaAPIWrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e32be59c-713b-4465-b58b-5f91ce3c1553",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain import LlamaCpp\n",
    "from langchain import PromptTemplate, LLMChain\n",
    "from langchain.agents import load_tools\n",
    "from langchain.agents import initialize_agent\n",
    "from langchain.agents import AgentType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "334eec3f-6c4d-4d7a-8839-08f89057f4de",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "66748d97-e819-4d87-923a-1b53f8aa282e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Constants definitions\n",
    "MQTT_BROKER = \"192.168.72.148\"  # MQTT IP Address\n",
    "MQTT_PORT = 1883  # MQTT Port\n",
    "MQTT_TOPIC = \"Marvin\"  # MQTT Topic \n",
    "MQTT_TOPIC_RECEIVE = \"MarvinGenerated\"  # MQTT Topic \n",
    "MQTT_USERNAME = \"marvin\"  # username\n",
    "MQTT_PASSWORD = \"secret\"  # password"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "01087a00-0e38-4dca-8819-566b25ed19e1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Wikipedia setup in French\n",
    "wikipedia.set_lang('fr')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "319b82e4-ed62-4353-9d5e-fe5c0c5b4074",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "WHITESPACE_HANDLER = lambda k: re.sub('\\s+', ' ', re.sub('\\n+', ' ', k.strip()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bde2e8b2-9176-4d6e-8844-d4a71d13fdbe",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Wikipedia function to get a summary for a subject\n",
    "def get_wikipedia_answer(Question):\n",
    "    wiki_summ=wikipedia.search(Question)\n",
    "    print(wiki_summ)\n",
    "    return wiki_summ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2a2334bc-949f-4884-b2b3-9f17a35a3660",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def create_response(Question):\n",
    "    llm = LlamaCpp(model_path=\"../../twitch/vigogne/llama.cpp/models/7B/ggml-model-q4_0.bin\",temperature=0.2,n_ctx=2048,max_tokens=1024)#,n_ctx=n_ctx,max_tokens=max_tokens\n",
    "    template = \"\"\"\n",
    "    Tu es un assistant expert dans les domaines demandés. Tu réponds avec précision et fiabilité\n",
    "    Réponds en français uniquement.\n",
    "    {Question}\n",
    "    \"\"\"\n",
    "    prompt = PromptTemplate(template=template, input_variables=[\"Question\"])\n",
    "    llm_chain = LLMChain(prompt=prompt, llm=llm)\n",
    "    reponse=llm_chain.predict(Question=Question)\n",
    "    return(\" **D'après mon modèle**\\n\"+reponse)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b64f437f-521a-4f00-ad0e-7d5710e25a61",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def create_wiki_response(Question):\n",
    "    reponse = ''\n",
    "    wikipedia = WikipediaAPIWrapper()\n",
    "    wiki_summ=get_wikipedia_answer(Question)\n",
    "    llm = LlamaCpp(model_path=\"../../twitch/vigogne/llama.cpp/models/7B/ggml-model-q4_0.bin\",temperature=0.2,n_ctx=1024,max_tokens=512)#,n_ctx=n_ctx,max_tokens=max_tokens)\n",
    "    if wiki_summ != []:\n",
    "        print(wiki_summ[0])\n",
    "        template = \"\"\"\n",
    "            {content}\n",
    "            {Question} \n",
    "        \"\"\"\n",
    "        prompt = PromptTemplate(template=template, input_variables=[\"Question\",\"content\"])\n",
    "        llm_chain = LLMChain(prompt=prompt, llm=llm)\n",
    "        sujet=wiki_summ[0].replace(\" \",\"_\")\n",
    "        page_summ = requests.get(f'https://fr.wikipedia.org/api/rest_v1/page/summary/{sujet}').json()[\"extract\"]\n",
    "        reponse=llm_chain.predict(wiki_summ=wiki_summ,Question=Question,content=page_summ)\n",
    "        #reponse = wiki_summ[0]\n",
    "        #print(dir(wikipedia.wiki_client.page(wiki_summ[0])))\n",
    "        reponse = \"\\n \\n **-------------------**\\n\" + reponse + \"\\n*Liens Wikipedia*\\n\"\n",
    "        for page in wiki_summ:\n",
    "            print(page)\n",
    "            try:\n",
    "                wikiurl=wikipedia.wiki_client.page(page).url\n",
    "            except:\n",
    "                wikiurl=''\n",
    "            reponse= reponse + '\\n' + wikiurl\n",
    "    return(reponse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e96a4dc5-fce7-4c3d-914e-230c1e1e7654",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#\n",
    "# callback function called each time a message is received on the topic\n",
    "#\n",
    "def on_message(client, userdata, message):\n",
    "    print(\"Received message on topic {}: {}\".format(message.topic, message.payload.decode()))\n",
    "    publish_response(message.payload.decode())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1c4886f2-4f69-4667-b8bb-0b5dc80a8a44",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#\n",
    "# We publish a message on another topic so the nodered\n",
    "# can retrieve our answer\n",
    "#\n",
    "def publish_response(message):\n",
    "    #\n",
    "    # Placeholder to generate a response\n",
    "    #\n",
    "    #response = get_wikipedia_answer(message)\n",
    "    response = create_response(message) + create_wiki_response(message)\n",
    "    #response = create_wiki_response(message)\n",
    "    client.publish(MQTT_TOPIC_RECEIVE, response)\n",
    "    print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "89c087a5-bed5-427c-8d6c-1133299c90cc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#create_response('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdacc669-55ad-46e6-b030-fe701876af0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Received message on topic Marvin: C'est quoi le théorème central limite ?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama.cpp: loading model from ../../twitch/vigogne/llama.cpp/models/7B/ggml-model-q4_0.bin\n",
      "llama_model_load_internal: format     = ggjt v1 (latest)\n",
      "llama_model_load_internal: n_vocab    = 32000\n",
      "llama_model_load_internal: n_ctx      = 2048\n",
      "llama_model_load_internal: n_embd     = 4096\n",
      "llama_model_load_internal: n_mult     = 256\n",
      "llama_model_load_internal: n_head     = 32\n",
      "llama_model_load_internal: n_layer    = 32\n",
      "llama_model_load_internal: n_rot      = 128\n",
      "llama_model_load_internal: ftype      = 2 (mostly Q4_0)\n",
      "llama_model_load_internal: n_ff       = 11008\n",
      "llama_model_load_internal: n_parts    = 1\n",
      "llama_model_load_internal: model size = 7B\n",
      "llama_model_load_internal: ggml ctx size =  59.11 KB\n",
      "llama_model_load_internal: mem required  = 5809.32 MB (+ 2052.00 MB per state)\n",
      "AVX = 1 | AVX2 = 1 | AVX512 = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 1 | VSX = 0 | \n",
      "llama_init_from_file: kv self size  = 2048.00 MB\n",
      "\n",
      "llama_print_timings:        load time =   549.64 ms\n",
      "llama_print_timings:      sample time =    30.41 ms /    51 runs   (    0.60 ms per run)\n",
      "llama_print_timings: prompt eval time =  3063.89 ms /    53 tokens (   57.81 ms per token)\n",
      "llama_print_timings:        eval time =  7724.56 ms /    50 runs   (  154.49 ms per run)\n",
      "llama_print_timings:       total time = 10824.34 ms\n",
      "llama.cpp: loading model from ../../twitch/vigogne/llama.cpp/models/7B/ggml-model-q4_0.bin\n",
      "llama_model_load_internal: format     = ggjt v1 (latest)\n",
      "llama_model_load_internal: n_vocab    = 32000\n",
      "llama_model_load_internal: n_ctx      = 1024\n",
      "llama_model_load_internal: n_embd     = 4096\n",
      "llama_model_load_internal: n_mult     = 256\n",
      "llama_model_load_internal: n_head     = 32\n",
      "llama_model_load_internal: n_layer    = 32\n",
      "llama_model_load_internal: n_rot      = 128\n",
      "llama_model_load_internal: ftype      = 2 (mostly Q4_0)\n",
      "llama_model_load_internal: n_ff       = 11008\n",
      "llama_model_load_internal: n_parts    = 1\n",
      "llama_model_load_internal: model size = 7B\n",
      "llama_model_load_internal: ggml ctx size =  59.11 KB\n",
      "llama_model_load_internal: mem required  = 5809.32 MB (+ 2052.00 MB per state)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Théorème central limite', 'Théorème du minimax de von Neumann', 'Intervalle de confiance', 'Théorème des deux carrés de Fermat', \"Équipartition de l'énergie\", 'Trou noir', 'Pythagore', 'Problème de Monty Hall', 'Emmy Noether', 'Pile ou face']\n",
      "Théorème central limite\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_init_from_file: kv self size  = 1024.00 MB\n",
      "AVX = 1 | AVX2 = 1 | AVX512 = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 1 | VSX = 0 | \n",
      "\n",
      "llama_print_timings:        load time =   556.40 ms\n",
      "llama_print_timings:      sample time =    69.19 ms /   116 runs   (    0.60 ms per run)\n",
      "llama_print_timings: prompt eval time =  5655.49 ms /    98 tokens (   57.71 ms per token)\n",
      "llama_print_timings:        eval time = 18189.02 ms /   115 runs   (  158.17 ms per run)\n",
      "llama_print_timings:       total time = 23928.19 ms\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Théorème central limite\n",
      "Théorème du minimax de von Neumann\n",
      "Intervalle de confiance\n",
      "Théorème des deux carrés de Fermat\n",
      "Équipartition de l'énergie\n",
      "Trou noir\n",
      "Pythagore\n",
      "Problème de Monty Hall\n",
      "Emmy Noether\n",
      "Pile ou face\n",
      " **D'après mon modèle**\n",
      "\n",
      "    Le théorème central limite affirme que si une série de données suit une loi normale, alors la moyenne et l'écart type des valeurs de cette série sont approximativement proportionnels à la variance.\n",
      " \n",
      " **-------------------**\n",
      "\n",
      "        Le théorème central limite est un résultat mathématique qui affirme que la somme de variables aléatoires indépendantes et identiquement distribuées tend vers une variable aléatoire gaussienne. C'est-à-dire, si vous ajoutez des quantités aléatoires simplement en fonction du temps (comme le nombre de pièces d'un certain type que vous avez acquises dans un certain mois), la somme des quantités tend vers une distribution normale.\n",
      "*Liens Wikipedia*\n",
      "\n",
      "https://fr.wikipedia.org/wiki/Th%C3%A9or%C3%A8me_central_limite\n",
      "https://fr.wikipedia.org/wiki/Th%C3%A9or%C3%A8me_du_minimax_de_von_Neumann\n",
      "https://fr.wikipedia.org/wiki/Intervalle_de_confiance\n",
      "https://fr.wikipedia.org/wiki/Th%C3%A9or%C3%A8me_des_deux_carr%C3%A9s_de_Fermat\n",
      "https://fr.wikipedia.org/wiki/%C3%89quipartition_de_l%27%C3%A9nergie\n",
      "\n",
      "https://fr.wikipedia.org/wiki/Pythagore\n",
      "https://fr.wikipedia.org/wiki/Probl%C3%A8me_de_Monty_Hall\n",
      "\n",
      "https://fr.wikipedia.org/wiki/Pile_ou_face\n",
      "Received message on topic Marvin: Donne moi un code python pour me connecter en wifi\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama.cpp: loading model from ../../twitch/vigogne/llama.cpp/models/7B/ggml-model-q4_0.bin\n",
      "llama_model_load_internal: format     = ggjt v1 (latest)\n",
      "llama_model_load_internal: n_vocab    = 32000\n",
      "llama_model_load_internal: n_ctx      = 2048\n",
      "llama_model_load_internal: n_embd     = 4096\n",
      "llama_model_load_internal: n_mult     = 256\n",
      "llama_model_load_internal: n_head     = 32\n",
      "llama_model_load_internal: n_layer    = 32\n",
      "llama_model_load_internal: n_rot      = 128\n",
      "llama_model_load_internal: ftype      = 2 (mostly Q4_0)\n",
      "llama_model_load_internal: n_ff       = 11008\n",
      "llama_model_load_internal: n_parts    = 1\n",
      "llama_model_load_internal: model size = 7B\n",
      "llama_model_load_internal: ggml ctx size =  59.11 KB\n",
      "llama_model_load_internal: mem required  = 5809.32 MB (+ 2052.00 MB per state)\n",
      "llama_init_from_file: kv self size  = 2048.00 MB\n",
      "AVX = 1 | AVX2 = 1 | AVX512 = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 1 | VSX = 0 | \n",
      "\n",
      "llama_print_timings:        load time =   551.42 ms\n",
      "llama_print_timings:      sample time =    33.22 ms /    56 runs   (    0.59 ms per run)\n",
      "llama_print_timings: prompt eval time =  3068.37 ms /    53 tokens (   57.89 ms per token)\n",
      "llama_print_timings:        eval time =  8519.95 ms /    55 runs   (  154.91 ms per run)\n",
      "llama_print_timings:       total time = 11627.51 ms\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Raspberry Pi', 'RISC-V', 'Universal Plug and Play', 'Kiwix', 'Liste des adoptants de GNU/Linux']\n",
      "Raspberry Pi\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama.cpp: loading model from ../../twitch/vigogne/llama.cpp/models/7B/ggml-model-q4_0.bin\n",
      "llama_model_load_internal: format     = ggjt v1 (latest)\n",
      "llama_model_load_internal: n_vocab    = 32000\n",
      "llama_model_load_internal: n_ctx      = 1024\n",
      "llama_model_load_internal: n_embd     = 4096\n",
      "llama_model_load_internal: n_mult     = 256\n",
      "llama_model_load_internal: n_head     = 32\n",
      "llama_model_load_internal: n_layer    = 32\n",
      "llama_model_load_internal: n_rot      = 128\n",
      "llama_model_load_internal: ftype      = 2 (mostly Q4_0)\n",
      "llama_model_load_internal: n_ff       = 11008\n",
      "llama_model_load_internal: n_parts    = 1\n",
      "llama_model_load_internal: model size = 7B\n",
      "llama_model_load_internal: ggml ctx size =  59.11 KB\n",
      "llama_model_load_internal: mem required  = 5809.32 MB (+ 2052.00 MB per state)\n",
      "llama_init_from_file: kv self size  = 1024.00 MB\n",
      "AVX = 1 | AVX2 = 1 | AVX512 = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 1 | VSX = 0 | \n",
      "\n",
      "llama_print_timings:        load time =   559.72 ms\n",
      "llama_print_timings:      sample time =   142.41 ms /   243 runs   (    0.59 ms per run)\n",
      "llama_print_timings: prompt eval time =  4834.74 ms /    84 tokens (   57.56 ms per token)\n",
      "llama_print_timings:        eval time = 38789.50 ms /   242 runs   (  160.29 ms per run)\n",
      "llama_print_timings:       total time = 43805.48 ms\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raspberry Pi\n",
      "RISC-V\n",
      "Universal Plug and Play\n",
      "Kiwix\n",
      "Liste des adoptants de GNU/Linux\n",
      " **D'après mon modèle**\n",
      "\n",
      "    # Code Python pour se connecter à Internet par Wi-Fi \n",
      "    import socket  \n",
      "    s = socket.socket()  \n",
      "    s.connect(('wifi', 1), 5)  \n",
      "    print(\"Se connecté\")\n",
      " \n",
      " **-------------------**\n",
      "\n",
      "    \"\"\"\n",
      "    \n",
      "    def __init__(self, mac_address=None):\n",
      "        self._mac_address = mac_address\n",
      "        if not self._mac_address:\n",
      "            raise ValueError('No MAC address provided')\n",
      "        \n",
      "    def connect(self, ssid, password):\n",
      "        \"\"\"Connect to the given WiFi network\"\"\"\n",
      "        \n",
      "        try:\n",
      "            wifi.sta.disconnect()\n",
      "            \n",
      "        except AttributeError:\n",
      "            pass\n",
      "        \n",
      "        try:\n",
      "            wifi.sta.config(ssid=ssid, password=password)\n",
      "            \n",
      "        except ValueError:\n",
      "            raise ValueError('WiFi SSID or password invalid')\n",
      "        \n",
      "        self._mac_address = wifi.sta.get_station()\n",
      "        \n",
      "    def disconnect(self):\n",
      "        \"\"\"Disconnect from the WiFi network\"\"\"\n",
      "        \n",
      "        try:\n",
      "            wifi.sta.disconnect()\n",
      "            \n",
      "        except AttributeError:\n",
      "            pass\n",
      "        \n",
      "        try:\n",
      "            wifi.sta.config(ssid='', password='')\n",
      "*Liens Wikipedia*\n",
      "\n",
      "https://fr.wikipedia.org/wiki/Raspberry_Pi\n",
      "https://fr.wikipedia.org/wiki/RISC-V\n",
      "https://fr.wikipedia.org/wiki/Universal_Plug_and_Play\n",
      "https://fr.wikipedia.org/wiki/Kiwix\n",
      "https://fr.wikipedia.org/wiki/Liste_des_adoptants_de_GNU/Linux\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "# Main loop\n",
    "#\n",
    "# Client MQTT initialisation\n",
    "client = mqtt.Client()\n",
    "client.username_pw_set(MQTT_USERNAME, MQTT_PASSWORD)  \n",
    "client.on_message = on_message\n",
    "client.connect(MQTT_BROKER, MQTT_PORT)\n",
    "client.subscribe(MQTT_TOPIC)\n",
    "\n",
    "# Loop to receive messages\n",
    "client.loop_forever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70126d77-0b06-4944-be1b-7170e342569c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c832479-3f8c-42e1-8703-fca30d83a6e3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
